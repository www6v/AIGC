{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "4spgQpFaGayQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Practical Experiment with Multi-Vector Retrieval: Exploring 3 Advanced Techniques in Table-Heavy Documents\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/https://colab.research.google.com/drive/16ZMcOtHU0hjfXP2lKnZV8gRcGuSJ6aRC?usp=sharing\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This notebook focuses on implementing and evaluating three Multi-Vector-Retriever approaches in table-rich documents from the perspective of practical work, it will not be an introductory tutorial on Multi-Vector-Retriever. The basic principles we have introduced in the first 2 videos([3种高级索引方法，有效提升RAG性能](https://www.bilibili.com/video/BV1dH4y1C7Ck/),  [【RAG实战】 Multi-Vector-Retrieval实现三种高级索引方法 （含Claude/GPT3-3.5评估结果）](https://www.bilibili.com/video/BV1Vu4y1H72s/)), you can click to learn.\n",
        "\n",
        "Three Methods with [MultiVectorRetriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector):\n",
        "\n",
        "\n",
        "**[Smaller chunks](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#smaller-chunks)**\n",
        "\n",
        "**[Summaries](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary)**\n",
        "\n",
        "**[Hypothetical Queries](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#hypothetical-queries)**\n",
        "\n",
        "**Tools we will use:**\n",
        "\n",
        "HTML documents parser: UnstructuredHTMLLoader\n",
        "\n",
        "MultiVectorRetriever\n",
        "\n",
        "[LCEL](https://python.langchain.com/docs/expression_language/) (Langchain Expression Language)"
      ],
      "metadata": {
        "id": "0TdhmVc5KmZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "1. Show experiments in real work, not an entry-level tutorial\n",
        "2. Running all of these methods will cost `$4-$8`. If you care, test them with some simple files."
      ],
      "metadata": {
        "id": "55xq6k2IJoos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "4spgQpFaGayQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMwZo0XQvBrU",
        "outputId": "e823ea13-5924-4dfd-c503-cd7a7dcd3b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.9/277.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m395.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain unstructured[all-docs] pydantic lxml langchainhub sentence_transformers chromadb openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the embeddings and model. We will use HuggingFacingEmbeddings(`all-MiniLM-L6-v2`) and `gpt-3.5-turbo-16k` model."
      ],
      "metadata": {
        "id": "Jl_sKaI2l7b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create folder results/ if not exists to save the experiment results.\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the folder path\n",
        "results_folder = \"results/\"\n",
        "\n",
        "# Check if the folder already exists\n",
        "if not os.path.exists(results_folder):\n",
        "    # Create the folder if it doesn't exist\n",
        "    os.makedirs(results_folder)\n",
        "    print(f\"Created folder: {results_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {results_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLSbCildCXs8",
        "outputId": "4616dbea-0d31-45ec-a541-731e2f7c8dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created folder: results/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.html import UnstructuredHTMLLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "from IPython.display import Markdown\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Supress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "# Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "\n",
        "# model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "model = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo-16k\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRTu6ru5lPzU",
        "outputId": "ce81b4b4-3c13-4499-a2d4-d7e0ef571edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "\n",
        "We will use the [Tesla-10-K-2022-Filing](https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm) which has plenty of tables and texts. You can download it.\n",
        "\n"
      ],
      "metadata": {
        "id": "aLQ28RwQm6ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "!wget https://www.dropbox.com/scl/fi/fqyvmodovgk21p06giezu/Tesla-10-K-2022-Filing.html ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilk6wqHnDZ4T",
        "outputId": "e56a5f58-3b02-4d65-8b96-4f60a6471c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-14 16:00:24--  https://www.dropbox.com/scl/fi/fqyvmodovgk21p06giezu/Tesla-10-K-2022-Filing.html\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.13.18, 2620:100:6035:18::a27d:5512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.13.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Tesla-10-K-2022-Filing.html’\n",
            "\n",
            "Tesla-10-K-2022-Fil     [ <=>                ]  68.51K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-12-14 16:00:24 (639 KB/s) - ‘Tesla-10-K-2022-Filing.html’ saved [70154]\n",
            "\n",
            "--2023-12-14 16:00:24--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2023-12-14 16:00:24--\n",
            "Total wall clock time: 0.4s\n",
            "Downloaded: 1 files, 69K in 0.1s (639 KB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import utils as chromautils\n",
        "\n",
        "# load data\n",
        "doc_path = \"./Tesla-10-K-2022-Filing.html\"\n",
        "\n",
        "loader = UnstructuredHTMLLoader(doc_path, mode=\"paged\")\n",
        "docs = loader.load()\n",
        "docs = chromautils.filter_complex_metadata(docs) # https://github.com/langchain-ai/langchain/issues/8556#issuecomment-1806835287\n",
        "\n",
        "# # rag baseline\n",
        "# data_texts = [element.page_content for element in docs]"
      ],
      "metadata": {
        "id": "rNX03zpwldRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Dataset\n",
        "\n",
        "```\n",
        "qna_dict = {\n",
        "        \"What is the value of cash and cash equivalents in 2022?\": \"16,253 $ millions\",\n",
        "        \"What is the value of cash and cash equivalents in 2021?\": \"17,576 $ millions\",\n",
        "        \"What is the net value of accounts receivable in 2022?\": \"2,952 $ millions\",\n",
        "        \"What is the net value of accounts receivable in 2021?\": \"1,913 $ millions\",\n",
        "        \"What is the total stockholders' equity? in 2022?\": \"44,704 $ millions\",\n",
        "        \"What is the total stockholders' equity? in 2021?\": \"30,189 $ millions\",\n",
        "        \"What are total operational expenses for research and development in 2022?\": \"3,075 $ millions\",\n",
        "        \"What are total operational expenses for research and development in 2021?\": \"2,593 $ millions\",\n",
        "    }\n",
        "```"
      ],
      "metadata": {
        "id": "PxxJAGRRm6dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qna_dict = {\n",
        "        \"What is the value of cash and cash equivalents in 2022?\": \"16,253 $ millions\",\n",
        "        \"What is the value of cash and cash equivalents in 2021?\": \"17,576 $ millions\",\n",
        "        \"What is the net value of accounts receivable in 2022?\": \"2,952 $ millions\",\n",
        "        \"What is the net value of accounts receivable in 2021?\": \"1,913 $ millions\",\n",
        "        \"What is the total stockholders' equity? in 2022?\": \"44,704 $ millions\",\n",
        "        \"What is the total stockholders' equity? in 2021?\": \"30,189 $ millions\",\n",
        "        \"What are total operational expenses for research and development in 2022?\": \"3,075 $ millions\",\n",
        "        \"What are total operational expenses for research and development in 2021?\": \"2,593 $ millions\",\n",
        "    }"
      ],
      "metadata": {
        "id": "Qq4wXMinokk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Function) Predict answer and evaluate"
      ],
      "metadata": {
        "id": "gZtcaISboLob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def predict_answer_and_evaluate(question, expected_answer, retriever, model):\n",
        "    \"\"\"\n",
        "      This function predicts an answer for a question using a LLM and then evaluates it against the expected answer.\n",
        "\n",
        "      Args:\n",
        "        question: The question to be answered.\n",
        "        expected_answer: The expected answer to the question.\n",
        "        retriever: The retriever used for retrieving relevant context.\n",
        "        model: The LLM model used for prediction and evaluation.\n",
        "\n",
        "      Returns:\n",
        "        A dictionary containing the following information:\n",
        "          - question: The original question.\n",
        "          - llm_answer: The answer predicted by the LLM.\n",
        "          - expected_answer: The expected answer to the question.\n",
        "          - is_correct: A string indicating whether the predicted answer matches the expected answer (\"Yes\") or not (\"No\").\n",
        "    \"\"\"\n",
        "\n",
        "    # Rag template for retrieving relevant context and prompting the LLM\n",
        "    rag_template = \"\"\"Answer the question based only on the following context:\n",
        "    {context}\n",
        "\n",
        "    Question: {question}\n",
        "    \"\"\"\n",
        "    rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "    # Parallel execution for context retrieval and question processing\n",
        "    setup_and_retrieval = RunnableParallel(\n",
        "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    )\n",
        "    # Chain the retrieval, prompting, prediction, and parsing steps\n",
        "    rag_chain = setup_and_retrieval | rag_prompt | model | StrOutputParser()\n",
        "\n",
        "    eval_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "            Input:\n",
        "            Question: {question}\n",
        "            LLM-Answer: {llm_answer}\n",
        "            Expected-Answer: {expected_answer}\n",
        "\n",
        "            Task:\n",
        "            Compare LLM-Answer and Expected-Answer and determine if they convey the same meaning or information.\n",
        "\n",
        "            Output:\n",
        "            Yes: If LLM-Answer and Expected-Answer convey the same meaning or information.\n",
        "            No: If LLM-Answer and Expected-Answer do not convey the same meaning or information.\n",
        "\n",
        "            Example:\n",
        "            Question: What is the capital of France?\n",
        "            LLM-Answer: Paris\n",
        "            Expected-Answer: The City of Lights\n",
        "\n",
        "            Output:\n",
        "            Yes\"\"\")\n",
        "    # Chain together the evaluation steps\n",
        "    eval_chain = (\n",
        "        {\n",
        "            \"llm_answer\": itemgetter(\"question\") | rag_chain,\n",
        "            \"question\": itemgetter(\"question\"),\n",
        "            \"expected_answer\": itemgetter(\"expected_answer\"),\n",
        "        }\n",
        "        | eval_template\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    # llm answer\n",
        "    llm_answer = rag_chain.invoke(question)\n",
        "\n",
        "    #output of evaluation chain\n",
        "    eval_answer = eval_chain.invoke({\"question\": question, \"expected_answer\": expected_answer})\n",
        "    # result\n",
        "    result = {\n",
        "    \"question\": question,\n",
        "    \"llm_answer\": llm_answer.strip(),\n",
        "    \"expected_answer\": expected_answer,\n",
        "    \"is_correct\": eval_answer\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "im_JKj10oOJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Baseline"
      ],
      "metadata": {
        "id": "jazD2jzToTXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# rag baseline\n",
        "data_texts = [element.page_content for element in docs]\n",
        "\n",
        "vectorstore = Chroma.from_texts(data_texts, collection_name=\"text_table\",\n",
        "                        embedding=embeddings)\n",
        "\n",
        "# retriever\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "zyo1v5RSoP7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "basic_answers = []\n",
        "for question in qna_dict.keys():\n",
        "    expected_answer = qna_dict[question]\n",
        "    # result\n",
        "    res = predict_answer_and_evaluate(question, expected_answer, retriever, model)\n",
        "    time.sleep(60) # resolve: Error Code 429\n",
        "    basic_answers.append(res)\n",
        "basic_answers_df = pd.DataFrame(basic_answers) # you may need to verify the is_correct manually.\n",
        "basic_answers_df.to_excel('results/rag_baseline_results.xlsx', index=False)"
      ],
      "metadata": {
        "id": "vnyevMnzoXAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smaller Chunks"
      ],
      "metadata": {
        "id": "7evkEVuQqm3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema.document import Document\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "import time\n",
        "t0=time.time()\n",
        "\n",
        "parent_chunk_size = 10000\n",
        "child_chunk_size = 400\n",
        "id_key = 'doc_id'\n",
        "collection_name = 'split_documents'\n",
        "\n",
        "# Create the parent documents\n",
        "parent_text_splitter = RecursiveCharacterTextSplitter(chunk_size=parent_chunk_size)\n",
        "docs = parent_text_splitter.split_documents(docs)\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=collection_name, embedding_function=embeddings\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = id_key\n",
        "# The retriever (empty to start)\n",
        "smaller_chunk_retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "# The splitter to use to create smaller chunks from parent documents\n",
        "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=child_chunk_size)\n",
        "\n",
        "sub_docs = []\n",
        "for i, doc in enumerate(docs):\n",
        "    _id = doc_ids[i]\n",
        "    # print(f'_id: {_id}\\n\\n')\n",
        "    _sub_docs = child_text_splitter.split_documents([doc])\n",
        "    for _doc in _sub_docs:\n",
        "        _doc.metadata[id_key] = _id\n",
        "        # print(f'_doc: {_doc}\\n\\n')\n",
        "    sub_docs.extend(_sub_docs)\n",
        "# print(sub_docs[0])\n",
        "# Add texts\n",
        "smaller_chunk_retriever.vectorstore.add_documents(sub_docs)\n",
        "smaller_chunk_retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
        "t1 = time.time()\n",
        "\n",
        "print(t1-t0, 's')"
      ],
      "metadata": {
        "id": "DhPlFP8borZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "smaller_chunk_answers = []\n",
        "for question in qna_dict.keys():\n",
        "    expected_answer = qna_dict[question]\n",
        "    # result\n",
        "    res = predict_answer_and_evaluate(question, expected_answer, smaller_chunk_retriever, model)\n",
        "    time.sleep(60) # resolve: Error Code 429\n",
        "    smaller_chunk_answers.append(res)\n",
        "\n",
        "smaller_chunk_answers_df = pd.DataFrame(smaller_chunk_answers)\n",
        "smaller_chunk_answers_df.to_excel('results/rag_smaller_chunk_results.xlsx', index=False)"
      ],
      "metadata": {
        "id": "klGkzb3loZP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summaries"
      ],
      "metadata": {
        "id": "fVd1j-v1otgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.schema.document import Document\n",
        "from langchain.storage import InMemoryStore\n",
        "import os\n",
        "\n",
        "# helper functions\n",
        "#to save time the summaries were pre-calculated\n",
        "TABLE_SUMMARIES_CSV = \"./table_summaries_0.csv\"\n",
        "\n",
        "def summarize(texts):\n",
        "    \"\"\"\n",
        "    This function summarizes the given texts using a GPT-3.5 model. It also checks if a CSV file with previous summaries exists,\n",
        "    if it does, it loads the summaries from there instead of generating new ones.\n",
        "\n",
        "    Args:\n",
        "        texts (list): A list of texts to be summarized.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of summarized texts.\n",
        "\n",
        "    \"\"\"\n",
        "    # Prompt\n",
        "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
        "    Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "    # Summary chain\n",
        "    # model = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo-16k-0613\", openai_api_key=open_ai_key)\n",
        "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "    tables = [i for i in texts]\n",
        "    table_summaries = []\n",
        "\n",
        "    # open csv file if it exists\n",
        "    if os.path.exists(TABLE_SUMMARIES_CSV):\n",
        "        t_frame = pd.read_csv(TABLE_SUMMARIES_CSV)\n",
        "        table_summaries = [elem[1] for elem in t_frame.values.tolist()]\n",
        "    else:\n",
        "        for i in range(0, len(tables)):\n",
        "            res = summarize_chain.invoke(tables[i])\n",
        "            table_summaries.append(res)\n",
        "\n",
        "        t_frame = pd.DataFrame(table_summaries)\n",
        "        t_frame.to_csv(TABLE_SUMMARIES_CSV)\n",
        "\n",
        "    return table_summaries\n",
        "\n",
        "def setup_retriever(sections):\n",
        "    \"\"\"\n",
        "    This function sets up a retriever for the given sections of text. It first summarizes the sections, then creates a\n",
        "    Chroma vectorstore to index the summaries. It also sets up an InMemoryStore for the parent documents and a\n",
        "    MultiVectorRetriever to retrieve the documents. Finally, it adds the summarized texts to the vectorstore and the\n",
        "    original sections to the docstore.\n",
        "\n",
        "    Args:\n",
        "        sections (list): A list of sections of text to be indexed and retrieved.\n",
        "\n",
        "    Returns:\n",
        "        MultiVectorRetriever: A retriever set up with the given sections of text.\n",
        "    \"\"\"\n",
        "    text_summaries = summarize(sections)\n",
        "    # The vectorstore to use to index the child chunks\n",
        "    vectorstore = Chroma(\n",
        "        collection_name=\"summaries\",\n",
        "\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    # The storage layer for the parent documents\n",
        "    store = InMemoryStore()\n",
        "    id_key = \"doc_id\"\n",
        "\n",
        "    # The retriever (empty to start)\n",
        "    retriever = MultiVectorRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        docstore=store,\n",
        "        id_key=id_key,\n",
        "    )\n",
        "\n",
        "    # Add texts\n",
        "    doc_ids = [str(uuid.uuid4()) for _ in sections]\n",
        "    summary_texts = [Document(page_content=s, metadata={id_key: doc_ids[i]}) for i, s in enumerate(text_summaries)]\n",
        "    retriever.vectorstore.add_documents(summary_texts)\n",
        "    retriever.docstore.mset(list(zip(doc_ids, sections)))\n",
        "\n",
        "    return retriever\n"
      ],
      "metadata": {
        "id": "KJtJt0-rstx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_texts = [element.page_content for element in docs]\n",
        "\n",
        "summary_retriever = setup_retriever(data_texts)"
      ],
      "metadata": {
        "id": "za2nChGCsu-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "summary_answers = []\n",
        "for question in qna_dict.keys():\n",
        "    expected_answer = qna_dict[question]\n",
        "    # result\n",
        "    res = predict_answer_and_evaluate(question, expected_answer, summary_retriever, model)\n",
        "    time.sleep(60) # resolve: Error Code 429\n",
        "    summary_answers.append(res)\n",
        "\n",
        "summary_answers_df = pd.DataFrame(summary_answers)\n",
        "summary_answers_df.to_excel('results/rag_summary_results.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "_FNESHuS1hXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypothetical Queries"
      ],
      "metadata": {
        "id": "HBWqKZXAs9Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import uuid\n",
        "\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.document import Document\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "import re\n",
        "\n",
        "hypo_query_chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Generate a list of exactly 3 hypothetical questions that the below document could be used to answer:\\n\\n{doc}\")\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: [re.sub(r\"\\d+\\. \", \"\", i) for i in x.strip().split(\"\\n\\n\")[1:]])\n",
        ")\n"
      ],
      "metadata": {
        "id": "91MmBcc9tBmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%timeit\n",
        "# hypothetical_questions = hypo_query_chain.batch(docs, {\"max_concurrency\": 5}) # failed\n",
        "\n",
        "# workaround\n",
        "hypothetical_questions = []\n",
        "for doc in docs:\n",
        "    hypo_queries = hypo_query_chain.invoke(doc, {\"max_concurrency\": 5})\n",
        "    hypothetical_questions.append(hypo_queries)"
      ],
      "metadata": {
        "id": "42DqMMhltC6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.schema.document import Document\n",
        "from langchain.storage import InMemoryStore\n",
        "\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name='hypo-queries', embedding_function=embeddings\n",
        ")\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "# The retriever (empty to start)\n",
        "hypo_questions_retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
        "\n",
        "question_docs = []\n",
        "for i, question_list in enumerate(hypothetical_questions):\n",
        "    question_docs.extend(\n",
        "        [Document(page_content=s, metadata={id_key: doc_ids[i]}) for s in question_list]\n",
        "    )\n",
        "\n",
        "hypo_questions_retriever.vectorstore.add_documents(question_docs)\n",
        "hypo_questions_retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ],
      "metadata": {
        "id": "nhOFyN5rtFYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "hypo_query_answers = []\n",
        "for question in qna_dict.keys():\n",
        "    expected_answer = qna_dict[question]\n",
        "    # result\n",
        "    res = predict_answer_and_evaluate(question, expected_answer, hypo_questions_retriever, model)\n",
        "    time.sleep(60) # resolve: Error Code 429\n",
        "    hypo_query_answers.append(res)\n",
        "\n",
        "hypo_query_answers_df = pd.DataFrame(hypo_query_answers)\n",
        "hypo_query_answers_df.to_excel('results/rag_hypo_query_results.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "cFYUIhjQtIK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Results\n"
      ],
      "metadata": {
        "id": "QEK-Orl-AcEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the results folder path\n",
        "results_folder = \"results/\"\n",
        "\n",
        "# List all Excel files in the folder\n",
        "excel_files = [f for f in os.listdir(results_folder) if f.endswith(\".xlsx\")]\n",
        "\n",
        "def add_method_column(filename):\n",
        "    \"\"\"\n",
        "    Adds a new column named 'method' to the loaded dataframe with the filename as the value.\n",
        "\n",
        "    Args:\n",
        "    filename: The name of the Excel file to be loaded.\n",
        "\n",
        "    Returns:\n",
        "    The Pandas DataFrame with the 'method' column added.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_excel(os.path.join(results_folder, filename))\n",
        "    method_name = os.path.splitext(filename)[0]\n",
        "    df[\"method\"] = method_name\n",
        "    return df\n",
        "\n",
        "# Combine all dataframes with added method column\n",
        "combined_df = pd.concat([add_method_column(f) for f in excel_files])\n",
        "# Save the combined dataframe to a new file\n",
        "combined_df.to_excel(\"experiment_results(raw).xlsx\", index=False)\n",
        "\n",
        "# Group the dataframe by method\n",
        "grouped_df = combined_df.groupby('method')\n",
        "# Calculate the accuracy for each method\n",
        "method_accuracy = combined_df.groupby('method')['is_correct'].value_counts(normalize=True) * 100\n",
        "\n",
        "method_accuracy = method_accuracy.reset_index()\n",
        "# Count the occurrences of \"Yes\" and \"No\"\n",
        "method_is_correct_count = grouped_df['is_correct'].value_counts().reset_index()#.unstack(fill_value=0)\n",
        "\n",
        "experiment_results = method_is_correct_count.merge(method_accuracy, on=['method', 'is_correct'])\n",
        "\n",
        "experiment_results.to_excel('experiment_results(agg).xlsx', index=False)"
      ],
      "metadata": {
        "id": "PhGukHOPBjGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}